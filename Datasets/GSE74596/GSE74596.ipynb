{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e661348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, LabelEncoder\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import iqr # For Silverman's rule\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# RLAC MODEL\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks  \n",
    "from sklearn.neighbors import KernelDensity  \n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from scipy import stats\n",
    "from scipy.special import eval_hermitenorm  # For normalized Hermite polynomials H_n(x)\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "import diptest\n",
    "from sklearn.metrics import (adjusted_mutual_info_score, adjusted_rand_score, \n",
    "                             homogeneity_score, completeness_score, v_measure_score,\n",
    "                             fowlkes_mallows_score, silhouette_score, calinski_harabasz_score,\n",
    "                             davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b8b3d-cd0c-48fa-9812-05365acba35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Local Loader\n",
    "import sc_loader as loader\n",
    "\n",
    "#  LOAD DATA\n",
    "DATA_DIR = r\"GSE74596 data\"\n",
    "SRA_PATH = r\"GSE74596 data\\SraRunTable (1).csv\"\n",
    "\n",
    "# Load using the robust local loader\n",
    "# We get a Matrix (Genes x Cells) and aligned Metadata\n",
    "raw_counts, metadata_df = loader.load_gse74596_data(DATA_DIR, SRA_PATH)\n",
    "\n",
    "# Inspect Labels\n",
    "print(\"\\n--- NKT Subsets Found ---\")\n",
    "if 'NKT_Subset' in metadata_df.columns:\n",
    "    print(metadata_df['NKT_Subset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcd41-079a-4c2a-bcec-87b0fc327fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_processor as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b4a6e-3a3e-44e9-b15d-22ca96d13337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  PROCESSING (Using Shared Module)\n",
    "# Config for RPKM data (similar to FPKM)\n",
    "QC_PARAMS = {\n",
    "    'min_tpm': 1,               # RPKM > 1\n",
    "    'min_genes_per_sample': 2000, \n",
    "    'min_samples_per_gene': 5\n",
    "}\n",
    "# 1. Filter\n",
    "filtered_df, qc_metrics = scp.filter_tpm_matrix(raw_df, **QC_PARAMS)\n",
    "\n",
    "# 2. Normalize (Log1p)\n",
    "log_df = scp.log_transform(filtered_df, method='log1p')\n",
    "\n",
    "# 3. Visualize Normalization\n",
    "scp.plot_expression_distribution(log_df, title=\"Log(RPKM+1) Distribution\")\n",
    "\n",
    "# 4. Feature Selection (HVG) & PCA\n",
    "df_hvg, hvg_metrics = scp.select_highly_variable_genes(log_df, n_top_genes=2000)\n",
    "df_scaled = scp.scale_data(df_hvg)\n",
    "df_pca, var_ratio, pca_model = scp.run_pca_pipeline(df_scaled, n_components=50)\n",
    "\n",
    "# Visualize PCA\n",
    "scp.plot_pca_results(df_pca, var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05820755-824f-4dfd-aad5-915d0003eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_clustering as scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67873a4c-e8a1-48fd-8172-d665d6b08b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CLUSTERING BENCHMARK\n",
    "# We define the columns we want to check against.\n",
    "TARGETS = ['NKT_Subset']\n",
    "\n",
    "# Ensure we have the data\n",
    "if 'df_pca' in locals() and 'metadata_df' in locals():\n",
    "    \n",
    "    # Run for a range of k\n",
    "    k_range = [3, 4, 5, 6] \n",
    "    clustering_results = scc.run_clustering_benchmark(\n",
    "        pca_df=df_pca, \n",
    "        cell_metadata=metadata_df, \n",
    "        n_clusters_range=k_range, \n",
    "        target_cols=TARGETS\n",
    "    )\n",
    "    # --- DISPLAY RESULTS ---\n",
    "    if not clustering_results.empty:\n",
    "        print(\"\\n=== Final Clustering Leaderboard ===\")\n",
    "        # We display the top 10 results\n",
    "        print(clustering_results.head(10))\n",
    "        \n",
    "        # Optional: Save to CSV for your portfolio evidence\n",
    "        # clustering_results.to_csv('clustering_benchmark_GSE52583.csv', index=False)\n",
    "       \n",
    "else:\n",
    "    print(\"Error: PCA or Metadata not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727729cc-719a-4123-90c9-8d2fa96612f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Get the path of the parent directory (Project_Root)\n",
    "# '..' means \"go up one level\"\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Add it to Python's search path if not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# 3. Now you can import normally\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "print(\"Successfully imported models from:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765a09f-6b52-4010-bb1f-44890a996c72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Import your custom models\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "X_train = pca_results_df\n",
    "y_train = qc_metadata_df['NKT_Subset']#.astype(str)\n",
    "n_clusters = y_train.nunique()\n",
    "\n",
    "print(f\"Number of Clusters (k): {n_clusters}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "rlac_methods = [\n",
    "    'depth_ratio', 'dip', 'holes', 'min_kurt', 'max_kurt', \n",
    "    'negentropy', 'skewness', 'fisher', 'hermite', 'friedman_tukey'\n",
    "]\n",
    "\n",
    "# Grid Search Parameters\n",
    "rlac_params = {\n",
    "    'random_state': [32, 42, 43, 44, 45],\n",
    "    'bw_adjust': [0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'r': [None, 50, 100, 300, 500]\n",
    "}\n",
    "\n",
    "# MDH Fixed Parameters\n",
    "mdh_config = {\n",
    "    \"h_multiplier\": 1.0,\n",
    "    \"alphamax_val\": 0.9,\n",
    "    \"alpha_steps\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# ==========================================\n",
    "# 3. RLAC LOOP\n",
    "# ==========================================\n",
    "print(f\"\\nStarting RLAC Grid Search ({len(rlac_methods) * len(rlac_params['r']) * len(rlac_params['bw_adjust']) * len(rlac_params['random_state'])} runs)...\")\n",
    "\n",
    "for method in rlac_methods:\n",
    "    for r_val in rlac_params['r']:\n",
    "        for bw in rlac_params['bw_adjust']:\n",
    "            for seed in rlac_params['random_state']:\n",
    "                \n",
    "                # Format r for display\n",
    "                r_str = \"JL\" if r_val is None else str(r_val)\n",
    "                param_str = f\"r={r_str}, bw={bw}, s={seed}\"\n",
    "                \n",
    "                print(f\"\\nRunning RLAC {method:<15} | {param_str} ... \", end=\"\")\n",
    "                \n",
    "                try:\n",
    "                    # Instantiate\n",
    "                    model = RLAC(\n",
    "                        n_clusters=n_clusters,\n",
    "                        method=method,\n",
    "                        r=r_val,\n",
    "                        bw_adjust=bw,\n",
    "                        random_state=seed,\n",
    "                        plot=False\n",
    "                    )\n",
    "                    \n",
    "                    # Fit\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        model.fit(X_train)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    ami = adjusted_mutual_info_score(y_train, model.labels_)\n",
    "                    ari = adjusted_rand_score(y_train, model.labels_)\n",
    "                    \n",
    "                    print(f\"Done (AMI: {ami:.4f})\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Model': 'RLAC',\n",
    "                        'Method': method,\n",
    "                        'Params': param_str,\n",
    "                        'AMI': ami,\n",
    "                        'ARI': ari\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"FAILED. Error: {e}\")\n",
    "                    results.append({\n",
    "                        'Model': 'RLAC', 'Method': method, 'Params': param_str,\n",
    "                        'AMI': -1, 'ARI': -1\n",
    "                    })\n",
    "\n",
    "# ==========================================\n",
    "# 4. MDH RUN\n",
    "# ==========================================\n",
    "print(\"-\" * 80)\n",
    "print(f\"Running MDH {'Standard':<15} | h=1.0, a=0.9 ... \", end=\"\")\n",
    "try:\n",
    "    mdh_model = MDH(\n",
    "        n_clusters=n_clusters,\n",
    "        h_multiplier=mdh_config['h_multiplier'],\n",
    "        alphamax_val=mdh_config['alphamax_val'],\n",
    "        alpha_steps=mdh_config['alpha_steps'],\n",
    "        random_state=mdh_config['random_state'],\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    mdh_model.fit(X_train)\n",
    "    ami_mdh = adjusted_mutual_info_score(y_train, mdh_model.labels_)\n",
    "    ari_mdh = adjusted_rand_score(y_train, mdh_model.labels_)\n",
    "    \n",
    "    print(f\"Done (AMI: {ami_mdh:.4f})\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'MDH',\n",
    "        'Method': 'Standard',\n",
    "        'Params': 'Fixed',\n",
    "        'AMI': ami_mdh,\n",
    "        'ARI': ari_mdh\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAILED. Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. RESULTS TABLE\n",
    "# ==========================================\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RESULTS FOR TARGET: {n_clusters} (Sorted by AMI)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort and display from best AMI to worst\n",
    "    results_df_sorted = results_df.sort_values(by='AMI', ascending=False)\n",
    "    print(results_df_sorted.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624727b-b0c0-4a37-a6f6-6346074376d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "================================================================================\n",
    "FINAL RESULTS FOR TARGET: 4 (Sorted by AMI)\n",
    "================================================================================\n",
    "Model         Method               Params       AMI       ARI\n",
    " RLAC          holes  r=300, bw=0.4, s=43  0.091578  0.010113\n",
    " RLAC          holes  r=300, bw=0.3, s=43  0.091578  0.010113\n",
    " RLAC          holes  r=300, bw=0.1, s=43  0.091578  0.010113\n",
    " RLAC          holes  r=300, bw=0.2, s=43  0.091578  0.010113\n",
    " RLAC            dip r=500, bw=0.05, s=32  0.083814 -0.005544\n",
    "  MDH       Standard                Fixed  0.077597  0.024191\n",
    " RLAC            dip  r=100, bw=0.1, s=32  0.071135  0.008852\n",
    " RLAC         fisher r=300, bw=0.05, s=43  0.069642  0.000548\n",
    " RLAC       min_kurt r=300, bw=0.05, s=43  0.069642  0.000548\n",
    " RLAC    depth_ratio   r=50, bw=0.4, s=43  0.066836  0.001160\n",
    " RLAC    depth_ratio   r=50, bw=0.2, s=43  0.066836  0.001160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aafb5f-609f-4b23-a93a-0fe27579e8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
