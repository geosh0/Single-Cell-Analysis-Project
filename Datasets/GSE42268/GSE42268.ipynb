{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a14b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, LabelEncoder\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import iqr # For Silverman's rule\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# RLAC MODEL\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks  \n",
    "from sklearn.neighbors import KernelDensity  \n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from scipy import stats\n",
    "from scipy.special import eval_hermitenorm  # For normalized Hermite polynomials H_n(x)\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import (adjusted_mutual_info_score, adjusted_rand_score, \n",
    "                             homogeneity_score, completeness_score, v_measure_score,\n",
    "                             fowlkes_mallows_score, silhouette_score, calinski_harabasz_score,\n",
    "                             davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c11b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sc_loader as loader # The new file above\n",
    "\n",
    "# --- Configuration ---\n",
    "METADATA_PATH = r\"\\SraRunTable.csv\"\n",
    "DATA_PATH = r\"GSE42268 data\"\n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "# This single line replaces the loop, pivot, and merge logic\n",
    "raw_counts_df, metadata_df = loader.load_gse42268_data(DATA_PATH, METADATA_PATH)\n",
    "\n",
    "# --- 2. INSPECT ---\n",
    "print(\"\\n--- Expression Matrix Head (Genes x Samples) ---\")\n",
    "print(raw_counts_df.iloc[:5, :5]) # Show first 5 genes and 5 samples\n",
    "\n",
    "print(\"\\n--- Metadata Head ---\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "# CHECK: Do the columns of the matrix match the rows of the metadata?\n",
    "if raw_counts_df.columns.equals(metadata_df.index):\n",
    "    print(\"\\nSUCCESS: Data and Metadata are perfectly aligned.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Alignment mismatch. Check sample IDs.\")\n",
    "    # Auto-align if needed\n",
    "    common = raw_counts_df.columns.intersection(metadata_df.index)\n",
    "    raw_counts_df = raw_counts_df[common]\n",
    "    metadata_df = metadata_df.loc[common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b776b-66e2-495c-89b0-2e6b1441cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_processor as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c2297-e7b0-432a-aa0d-1f9ca0af480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SETUP & CONFIGURATION\n",
    "QC_CONFIG = {\n",
    "    'min_tpm': 1,              # FPKM > 1 is a standard detection threshold\n",
    "    'min_genes_per_sample': 1000, # High quality Smart-Seq usually has >2000 genes\n",
    "    'min_samples_per_gene': 2\n",
    "}\n",
    "\n",
    "#  FILTERING & NORMALIZATION\n",
    "if 'raw_counts_df' in locals():\n",
    "    \n",
    "    # Filter, Note: The function says 'tpm', but it works mathematically identical for FPKM\n",
    "    filtered_df, qc_metrics = proc.filter_tpm_matrix(\n",
    "        raw_counts_df, \n",
    "        **QC_CONFIG\n",
    "    )\n",
    "    # Log Transform (log(FPKM + 1))\n",
    "    log_df = proc.log_transform(filtered_df, method='log1p')\n",
    "\n",
    "    # Visual Check\n",
    "    print(\"\\n--- QC Summary ---\")\n",
    "    print(qc_metrics.describe())\n",
    "    proc.plot_expression_distribution(log_df, title=\"Log(FPKM+1) Distribution\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: raw_counts_df not found. Please run the Loading step first.\")\n",
    "\n",
    "# 3. FEATURE SELECTION & PCA\n",
    "if 'log_df' in locals() and not log_df.empty:\n",
    "    \n",
    "    # 1. Select Highly Variable Genes (HVG)\n",
    "    df_hvg, hvg_metrics = proc.select_highly_variable_genes(log_df, n_top_genes=3000)\n",
    "    proc.plot_hvg_dispersion(hvg_metrics)\n",
    "\n",
    "    # 2. Scale Data (Z-score)\n",
    "    df_scaled = proc.scale_data(df_hvg)\n",
    "\n",
    "    # 3. Run PCA\n",
    "    df_pca, var_ratio, pca_model = proc.run_pca_pipeline(df_scaled, n_components=50)\n",
    "\n",
    "    # 4. Visualize Results\n",
    "    proc.plot_pca_results(df_pca, var_ratio)\n",
    "    \n",
    "    print(\"PCA processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85816bae-9b4f-4575-b3dc-8ab3ea8cf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_clustering as scc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5aef5-f1f7-48ce-9c22-58958cd8f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CLUSTERING BENCHMARK\n",
    "TARGET_COL = 'cell_type' \n",
    "\n",
    "# Check if the column exists in our metadata\n",
    "if 'metadata_df' in locals() and TARGET_COL not in metadata_df.columns:\n",
    "    print(f\"Warning: '{TARGET_COL}' not found. Switching to 'Filename_Group'.\")\n",
    "    TARGET_COL = 'Filename_Group'\n",
    "\n",
    "if 'df_pca' in locals() and 'metadata_df' in locals():\n",
    "    \n",
    "    print(f\"Running Benchmark against Ground Truth: {TARGET_COL}\")\n",
    "    \n",
    "    # Define range of K to test\n",
    "    # (e.g., if we expect 3 cell types, we test 2, 3, 4, 5)\n",
    "    k_range = [2, 3, 4, 5, 6]\n",
    "\n",
    "    clustering_results = cluster.run_baseline_clustering(\n",
    "        pca_df=df_pca, \n",
    "        cell_metadata=metadata_df, \n",
    "        n_clusters_range=k_range,\n",
    "        true_label_col=TARGET_COL\n",
    "    )\n",
    "    # --- DISPLAY LEADERBOARD ---\n",
    "    if not clustering_results.empty:\n",
    "        print(\"\\n=== Final Clustering Leaderboard ===\")\n",
    "        print(clustering_results)\n",
    "        \n",
    "        # Grab the best result\n",
    "        best = clustering_results.iloc[0]\n",
    "        print(f\"\\nWinner: {best['Method']} with k={best['k']} (AMI={best['AMI']:.3f})\")\n",
    "else:\n",
    "    print(\"Required data (PCA or Metadata) is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c5e76-74f8-4553-8a2b-5f5d59e41bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Get the path of the parent directory (Project_Root)\n",
    "# '..' means \"go up one level\"\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Add it to Python's search path if not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# 3. Now you can import normally\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "print(\"Successfully imported models from:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a06f2-56d2-4dee-8331-088bee95bdfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pca_df\n",
    "y_train = metadata_filtered['cell_type']\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "# Import your custom models\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "n_clusters = len(set(y_train))\n",
    "\n",
    "# RLAC Parameters\n",
    "rlac_methods = [\n",
    "    'depth_ratio', 'dip', 'holes', 'min_kurt', 'max_kurt', \n",
    "    'negentropy', 'skewness', 'fisher', 'hermite', 'friedman_tukey'\n",
    "]\n",
    "rlac_params = {\n",
    "    'random_state': [32, 42, 43, 44, 45],\n",
    "    'bw_adjust': [0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'r': [None, 50, 100, 200, 300, 500] \n",
    "}\n",
    "\n",
    "# MDH Parameters\n",
    "mdh_config = {\n",
    "    \"h_multiplier\": 1.0,\n",
    "    \"alphamax_val\": 0.9,\n",
    "    \"alpha_steps\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nStarting Benchmark on Single Cell Data (n={len(X_train)}, k={n_clusters})...\")\n",
    "print(f\"\\nTotal RLAC Combinations: {len(rlac_methods) * len(rlac_params['r']) * len(rlac_params['bw_adjust']) * len(rlac_params['random_state'])}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ==========================================\n",
    "# 1. RLAC LOOP\n",
    "# ==========================================\n",
    "for method in rlac_methods:\n",
    "    for r_val in rlac_params['r']:\n",
    "        for bw in rlac_params['bw_adjust']:\n",
    "            for seed in rlac_params['random_state']:\n",
    "                \n",
    "                param_str = f\"r={r_val}, bw={bw}, s={seed}\"\n",
    "                # Using end=\"\" to keep the line until 'Done' is printed\n",
    "                print(f\"\\n\\rRunning RLAC {method:<15} | {param_str} ... \", end=\"\")\n",
    "                \n",
    "                try:\n",
    "                    model = RLAC(\n",
    "                        n_clusters=n_clusters,\n",
    "                        method=method,\n",
    "                        r=r_val,\n",
    "                        bw_adjust=bw,\n",
    "                        random_state=seed,\n",
    "                        plot=False\n",
    "                    )\n",
    "                    \n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        model.fit(X_train)\n",
    "                    \n",
    "                    ami = adjusted_mutual_info_score(y_train, model.labels_)\n",
    "                    ari = adjusted_rand_score(y_train, model.labels_)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Model': 'RLAC',\n",
    "                        'Method': method,\n",
    "                        'Params': param_str,\n",
    "                        'AMI': ami,\n",
    "                        'ARI': ari\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Fail silently in the loop to keep output clean, but record failure\n",
    "                    results.append({\n",
    "                        'Model': 'RLAC', 'Method': method, 'Params': param_str,\n",
    "                        'AMI': -1, 'ARI': -1\n",
    "                    })\n",
    "\n",
    "print(\"\\nRLAC Loop Complete.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MDH RUN\n",
    "# ==========================================\n",
    "print(f\"\\nRunning MDH {'Standard':<15} | h=1.0, a=0.9 ... \")\n",
    "try:\n",
    "    mdh_model = MDH(\n",
    "        n_clusters=n_clusters,\n",
    "        h_multiplier=mdh_config['h_multiplier'],\n",
    "        alphamax_val=mdh_config['alphamax_val'],\n",
    "        alpha_steps=mdh_config['alpha_steps'],\n",
    "        random_state=mdh_config['random_state'],\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    mdh_model.fit(X_train)\n",
    "    \n",
    "    ami_mdh = adjusted_mutual_info_score(y_train, mdh_model.labels_)\n",
    "    ari_mdh = adjusted_rand_score(y_train, mdh_model.labels_)\n",
    "    \n",
    "    print(f\"Done (AMI: {ami_mdh:.4f})\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'MDH',\n",
    "        'Method': 'Standard',\n",
    "        'Params': 'Fixed',\n",
    "        'AMI': ami_mdh,\n",
    "        'ARI': ari_mdh\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"MDH FAILED. Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. RESULTS TABLE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS (ALL MODELS - SORTED BY AMI)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DataFrame and sort\n",
    "results_df = pd.DataFrame(results).sort_values(by='AMI', ascending=False)\n",
    "\n",
    "# Print everything\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf424ca-28f4-44ea-91c7-c53d7aa1d8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
