{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e661348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, LabelEncoder\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import iqr # For Silverman's rule\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# RLAC MODEL\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks  \n",
    "from sklearn.neighbors import KernelDensity  \n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from scipy import stats\n",
    "from scipy.special import eval_hermitenorm  # For normalized Hermite polynomials H_n(x)\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "import diptest\n",
    "from sklearn.metrics import (adjusted_mutual_info_score, adjusted_rand_score, \n",
    "                             homogeneity_score, completeness_score, v_measure_score,\n",
    "                             fowlkes_mallows_score, silhouette_score, calinski_harabasz_score,\n",
    "                             davies_bouldin_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c562b8-ec91-4a59-803e-57abef72b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sc_loader as loader  # Local File Parsing\n",
    "\n",
    "#  LOAD DATA\n",
    "DATA_DIR = r'GSE65528 data'\n",
    "SERIES_MATRIX = r'GSE65528 data\\GSE65528_series_matrix.txt'\n",
    "\n",
    "# Load\n",
    "raw_tpm, metadata_df = loader.load_gse65528_data(DATA_DIR, SERIES_MATRIX)\n",
    "\n",
    "# Inspect\n",
    "print(\"\\n--- Metadata Summary ---\")\n",
    "print(metadata_df['Combined_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01f337-d74e-44a9-aee4-5a08a25accfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_processor as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8cf0e-e83b-44e1-8b4c-71bebe891c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PROCESSING\n",
    "# Standard Pipeline\n",
    "# Note: Data is TPM, so we just filter and Log\n",
    "QC_PARAMS = {'min_tpm': 1, 'min_genes_per_sample': 2000, 'min_samples_per_gene': 3}\n",
    "\n",
    "filtered_df, qc_metrics = scp.filter_tpm_matrix(raw_tpm, **QC_PARAMS)\n",
    "log_df = scp.log_transform(filtered_df, method='log1p')\n",
    "df_hvg, hvg_metrics = scp.select_highly_variable_genes(log_df, n_top_genes=2000)\n",
    "df_scaled = scp.scale_data(df_hvg)\n",
    "df_pca, var_ratio, pca_model = scp.run_pca_pipeline(df_scaled, n_components=50)\n",
    "\n",
    "# Visualize\n",
    "scp.plot_pca_results(df_pca, var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133fcb2-a8b6-4c58-bb93-dbfaa947922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROFESSIONAL IMPORT SETUP\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define path to the shared_utils folder (one level up)\n",
    "shared_path = os.path.abspath(os.path.join(current_dir, '..', 'shared_utils'))\n",
    "\n",
    "# Add to system path if not already there\n",
    "if shared_path not in sys.path:\n",
    "    sys.path.append(shared_path)\n",
    "\n",
    "import sc_clustering as scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da25ae3-7a82-421e-ba5a-babdf36e9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CLUSTERING BENCHMARK\n",
    "# We test against Time, Status, AND the combination of both\n",
    "TARGETS = ['TimePoint', 'Infection_Status', 'Combined_Label']\n",
    "\n",
    "# Ensure we have the data\n",
    "if 'df_pca' in locals() and 'metadata_df' in locals():\n",
    "    \n",
    "    # Run for a range of k\n",
    "    # We include 13 because the paper might mention specific stages\n",
    "    k_range = [3, 4, 5] \n",
    "    \n",
    "    clustering_results = scc.run_clustering_benchmark(\n",
    "        pca_df=df_pca, \n",
    "        cell_metadata=metadata_df, \n",
    "        n_clusters_range=k_range, \n",
    "        target_cols=TARGETS\n",
    "    )\n",
    "\n",
    "    # --- DISPLAY RESULTS ---\n",
    "    if not clustering_results.empty:\n",
    "        print(\"\\n=== Final Clustering Leaderboard ===\")\n",
    "        # We display the top 10 results\n",
    "        print(clustering_results.head(10))\n",
    "        \n",
    "        # Optional: Save to CSV for your portfolio evidence\n",
    "        # clustering_results.to_csv('clustering_benchmark_GSE45719.csv', index=False)\n",
    "        \n",
    "else:\n",
    "    print(\"Error: PCA or Metadata not found. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d1822-a307-445c-becf-289fb726baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Get the path of the parent directory (Project_Root)\n",
    "# '..' means \"go up one level\"\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Add it to Python's search path if not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# 3. Now you can import normally\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "print(\"Successfully imported models from:\", parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad3645-2ec7-4d11-8c40-e93afcb3df9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "# Import your custom models\n",
    "from rlac import RLAC\n",
    "from mdh import MDH\n",
    "\n",
    "X_train = pca_results_df\n",
    "y_train = filtered_cell_metadata['TimePoint_h'].astype(str)\n",
    "n_clusters = y_train.nunique()\n",
    "\n",
    "print(f\"Number of Clusters (k): {n_clusters}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "rlac_methods = [\n",
    "    'depth_ratio', 'dip', 'holes', 'min_kurt', 'max_kurt', \n",
    "    'negentropy', 'skewness', 'fisher', 'hermite', 'friedman_tukey'\n",
    "]\n",
    "\n",
    "# Grid Search Parameters\n",
    "rlac_params = {\n",
    "    'random_state': [32, 42, 43, 44, 45],\n",
    "    'bw_adjust': [0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "    'r': [None, 50, 100, 300, 500]\n",
    "}\n",
    "\n",
    "# MDH Fixed Parameters\n",
    "mdh_config = {\n",
    "    \"h_multiplier\": 1.0,\n",
    "    \"alphamax_val\": 0.9,\n",
    "    \"alpha_steps\": 5,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# ==========================================\n",
    "# 3. RLAC LOOP\n",
    "# ==========================================\n",
    "print(f\"\\nStarting RLAC Grid Search ({len(rlac_methods) * len(rlac_params['r']) * len(rlac_params['bw_adjust']) * len(rlac_params['random_state'])} runs)...\")\n",
    "\n",
    "for method in rlac_methods:\n",
    "    for r_val in rlac_params['r']:\n",
    "        for bw in rlac_params['bw_adjust']:\n",
    "            for seed in rlac_params['random_state']:\n",
    "                \n",
    "                # Format r for display\n",
    "                r_str = \"JL\" if r_val is None else str(r_val)\n",
    "                param_str = f\"r={r_str}, bw={bw}, s={seed}\"\n",
    "                \n",
    "                print(f\"\\nRunning RLAC {method:<15} | {param_str} ... \", end=\"\")\n",
    "                \n",
    "                try:\n",
    "                    # Instantiate\n",
    "                    model = RLAC(\n",
    "                        n_clusters=n_clusters,\n",
    "                        method=method,\n",
    "                        r=r_val,\n",
    "                        bw_adjust=bw,\n",
    "                        random_state=seed,\n",
    "                        plot=False\n",
    "                    )\n",
    "                    \n",
    "                    # Fit\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                        model.fit(X_train)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    ami = adjusted_mutual_info_score(y_train, model.labels_)\n",
    "                    ari = adjusted_rand_score(y_train, model.labels_)\n",
    "                    \n",
    "                    print(f\"Done (AMI: {ami:.4f})\")\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Model': 'RLAC',\n",
    "                        'Method': method,\n",
    "                        'Params': param_str,\n",
    "                        'AMI': ami,\n",
    "                        'ARI': ari\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"FAILED. Error: {e}\")\n",
    "                    results.append({\n",
    "                        'Model': 'RLAC', 'Method': method, 'Params': param_str,\n",
    "                        'AMI': -1, 'ARI': -1\n",
    "                    })\n",
    "\n",
    "# ==========================================\n",
    "# 4. MDH RUN\n",
    "# ==========================================\n",
    "print(\"-\" * 80)\n",
    "print(f\"Running MDH {'Standard':<15} | h=1.0, a=0.9 ... \", end=\"\")\n",
    "try:\n",
    "    mdh_model = MDH(\n",
    "        n_clusters=n_clusters,\n",
    "        h_multiplier=mdh_config['h_multiplier'],\n",
    "        alphamax_val=mdh_config['alphamax_val'],\n",
    "        alpha_steps=mdh_config['alpha_steps'],\n",
    "        random_state=mdh_config['random_state'],\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "    \n",
    "    mdh_model.fit(X_train)\n",
    "    ami_mdh = adjusted_mutual_info_score(y_train, mdh_model.labels_)\n",
    "    ari_mdh = adjusted_rand_score(y_train, mdh_model.labels_)\n",
    "    \n",
    "    print(f\"Done (AMI: {ami_mdh:.4f})\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'MDH',\n",
    "        'Method': 'Standard',\n",
    "        'Params': 'Fixed',\n",
    "        'AMI': ami_mdh,\n",
    "        'ARI': ari_mdh\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FAILED. Error: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. RESULTS TABLE\n",
    "# ==========================================\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RESULTS FOR TARGET: {n_clusters} (Sorted by AMI)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort and display from best AMI to worst\n",
    "    results_df_sorted = results_df.sort_values(by='AMI', ascending=False)\n",
    "    print(results_df_sorted.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47360635-b4c9-4f00-b969-b35177c54722",
   "metadata": {},
   "outputs": [],
   "source": [
    "================================================================================\n",
    "FINAL RESULTS FOR TARGET: 4 (Sorted by AMI)\n",
    "================================================================================\n",
    "Model         Method               Params       AMI       ARI\n",
    " RLAC       skewness  r=500, bw=0.1, s=43  0.251717  0.180310\n",
    " RLAC       skewness r=500, bw=0.05, s=43  0.251717  0.180310\n",
    " RLAC    depth_ratio   r=JL, bw=0.4, s=32  0.214503  0.126993\n",
    " RLAC    depth_ratio r=500, bw=0.05, s=44  0.214196  0.107437\n",
    " RLAC        hermite  r=300, bw=0.3, s=45  0.199606  0.134232\n",
    " RLAC        hermite  r=300, bw=0.2, s=45  0.199606  0.134232\n",
    " RLAC    depth_ratio  r=JL, bw=0.05, s=32  0.193110  0.126934\n",
    " RLAC friedman_tukey   r=JL, bw=0.2, s=42  0.189521  0.111926\n",
    " RLAC friedman_tukey   r=JL, bw=0.1, s=42  0.189521  0.111926\n",
    " RLAC friedman_tukey  r=JL, bw=0.05, s=42  0.189521  0.111926\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8541a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
